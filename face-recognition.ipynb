{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TENSORFLOW\n",
    "\n",
    "LIBRERIA MATEMATICA OPEN-SOURCE DI GOOGLE\n",
    "PER SEMPLIFICARE L'UTILIZZO DI TECNICHE DI MACHINE LEARNING\n",
    "DA USARE NELLE RETI NEURALI E IN TUTTE LE SUE VARIANTI\n",
    "CHE SONO GIA' IMPLEMENTATE IN QUESTA LIBRERIA\n",
    "\n",
    "INOLTRE PERMETTE DI COSTRUIRE MODELLI DI RETI NEURALI E TUTTE LE SUE VARIANTI\n",
    "CHE UTILIZZANO QUESTI ALGORITMI DI MACHINE LEARNING COME BACK PROPAGATION\n",
    "UTILI A IMPARARE DURANTE IL TRAINING CERCANDO DI MINIMIZZARE IL GRADIENTE\n",
    "DELLA FUNZIONE A PIU' VARIABILI DEFINITA DALLA RETE NEURALE\n",
    "\n",
    "I MODELLI DI RETI NEURALI SONO DEI GRAFI DOVE:\n",
    "- NODI\n",
    "  NEURONI CHE RAPPRESENTANO UN'OPERAZIONE MATEMATICA CHE NORMALIZZA L'OUTPUT RICEVUTO\n",
    "- ARCHI\n",
    "  SINAPSI CHE RAPPRESENTANO I TENSORI, CIOE' UN ARRAY MULTIDIMENSIONALE\n",
    "\n",
    "E' UNA LIBRERIA PYTHON, LINGUAGGIO CON UN'ASTRAZIONE AD ALTO LIVELLO SIMILE ALL'INGLESE,\n",
    "CHE PERO' FUNGE SOLO DA INTERPRETE TRA L'UTENTE\n",
    "E IL FRAMEWORK CONTENENTE LE OPERAZIONI MATEMATICHE SCRITTO IN C++\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "KERAS API\n",
    "\n",
    "HIGH-LEVEL API FORNITA DA TENSORFLOW 2.0\n",
    "CHE PERMETTE DI COSTRUIRE UN MODELLO DI UNA RETE NEURALE\n",
    "IN MODO PIU' SEMPLICE RISPETTO ALLE PRECEDENTI\n",
    "\n",
    "ESSA SUPPORTA MOLTI TIPI DI MOTORI DI CALCOLO PER IL BACK-END DELLE RETI NEURALI\n",
    "\n",
    "RISPETTO A TENSORFLOW CORE API E' PIU' AD ALTO LIVELLO\n",
    "ED ACCESSIBILE ANCHE A CHI E' MENO ESPERTO\n",
    "ANCHE SE PROGRAMMARE A BASSO LIVELLO CONSENTE DI:\n",
    "- GESTIRE PIU' IN DETTAGLIO LA CURVA DI APPRENDIMENTO\n",
    "- MIGLIORE DEBUGGING\n",
    "MA SI PUO' FARE ANCHE UN MIX TRA I DUE\n",
    "\n",
    "KERAS E':\n",
    "- USER-FRIENDLY\n",
    "- SEGUE LE MIGLIORI PRATICHE DI MACHINE LEARNING PER RIDURRE IL CARICO COGNITIVO\n",
    "- MODULARE\n",
    "\n",
    "MODULI PRE-PROGRAMMATI (STANDALONE):\n",
    "- NEURAL LAYERS\n",
    "- COST FUNCTIONS\n",
    "- OPTIMIZERS\n",
    "- INITIALIZATION SCHEMES\n",
    "- ACTIVATION FUNCTIONS\n",
    "- REGULARIZATION SCHEMES\n",
    "\n",
    "SCRITTA IN PYTHON\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "KERAS BACK-END\n",
    "\n",
    "KERAS NON SI OCCUPA DI FARE OPERAZIONI A BASSO LIVELLO\n",
    "COME DOT PRODUCT TRA TENSORI E CONVOLUZIONI\n",
    "DELEGA QUESTE OPERAZIONI AD UN BACK-END ENGINE\n",
    "SUPPORTA MOLTI TIPI DI BACKEND MA QUELLO PRINCIPALE DI CUI E' API E' TENSORFLOW\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "KERAS MODEL\n",
    "\n",
    "STRUTTURA DATI PRINCIPALE DI KERAS, ESISTONO 2 TIPI:\n",
    "- SEQUENTIAL MODEL\n",
    "- MODEL CLASS\n",
    "\n",
    "SEQUENTIAL MODEL\n",
    "\n",
    "PREVEDE UNA STACK DI LAYERS LINEARE\n",
    "DOVE LE ACTIVATION, LOSS FUNCTIONS POSSONO ESSERE CHIAMATE IN FORMATO STRINGA\n",
    "COMPOSTO DA:\n",
    "- model.fit()\n",
    "  FA L'ALLENAMENTO IN GRUPPI (FITTING)\n",
    "- model.evaluate()\n",
    "  CALCOLA LE METRICHE E LE PERDITE PER IL MODELLO GIA' ALLENATO\n",
    "- model.predict()\n",
    "  PRENDE UN INPUT, LO SOTTOPONE AL MODELLO ALLENATO, GENERA UN OUTPUT CON UNA ETICHETTA\n",
    "\n",
    "MODEL CLASS - KERAS FUNCTIONAL API\n",
    "\n",
    "UTILE PER CREARE MODELLI COMPLESSI\n",
    "CHE PRENDONO MOLTI INPUT E FORNISCONO MOLTI OUTPUT\n",
    "DIRECTED ACYCLIC GRAPH (DAG)\n",
    "E MODELLI CON LAYERS CONDIVISI\n",
    "\n",
    "USA LO STESSO TIPO DI LATER DEL MODELLO SEQUENZIALE MA E' PIU' FLESSIBILE\n",
    "PRIMA SI DEFINISCONO I LAYERS\n",
    "POI SI CREA IL MODELLO\n",
    "POI SI COMPILA\n",
    "POI SI ALLENA\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "KERAS LAYERS\n",
    "\n",
    "DENSE LAYER\n",
    "- DOT PRODUCT DEI TENSORI PIU' BIAS (DENSE NEURAL NETWORK)\n",
    "\n",
    "ACTIVATION LAYERS\n",
    "- FUNZIONI DI TRASFERIMENTO PER MODELLARE I NEURONI\n",
    "\n",
    "DROPOUT LAYERS\n",
    "- IMPOSTA IN MODO CASUALE DELLE PORZIONI DELL'INPUT A 0\n",
    "  PER OGNI EPOCA PER EVITARE L'OVERFITTING\n",
    "\n",
    "LAMBDA LAYERS\n",
    "- CONVERTE UN'ESPRESSIONE IN UN LAYER\n",
    "\n",
    "CONVOLUTION LAYERS (1D, 2D , 3D)\n",
    "- PROCESSO DI CONVOLUZIONE CON UN FILTRO PER IL RICONOSCIMENTO E LA CREAZIONE DI UNA MAPPA DELLE FEATURES (CNN)\n",
    "- CONV2D E' ISPIRATA DALLA CORTECCIA PREFRONTALE ED E' USATA PER L'IMAGE RECOGNITION\n",
    "\n",
    "POOLING (DOWNSCALING) LAYERS (1D, 2D, 3D)\n",
    "- PROCESSO DI RIDUZIONE DELLA DIMENSIONE DELL'IMMAGINE PER EVITARE L'OVERFITTING ED OTTENERE IL PATTERN NECESSARIO AL RICONOSCIMENTO\n",
    "COMPOSTO DA:\n",
    "- MAX POOLING\n",
    "  FA UN PROCESSO DI CONVOLUZIONE CON UN FILTRO NxN DOVE VIENE PRESO IL VALORE MAGGIORE PER OGNI BOUNDING BOX\n",
    "- AVERAGE POOLING\n",
    "  FA UN PROCESSO DI CONVOLUZIONE CON UN FILTRO NxN DOVE VIENE CALCOLATO IL VALORE MEDIO PER OGNI BOUNDING BOX\n",
    "\n",
    "LOCALLY CONNECTED LAYERS\n",
    "- COME CONVOLUTIONAL LAYERS MA I PESI NON SONO CONDIVISI\n",
    "\n",
    "NOISE LAYERS\n",
    "- AVOID OVERFITTING\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LIBRERIA CHE PERMETTE DI SVOLGERE OPERAZIONI SUL SISTEMA OPERATIVO\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "from shutil import copyfile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import math\n",
    "\n",
    "# LIBRERIA PER LE OPERAZIONI SUGLI OGGETTI JSON\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PERCORSO DEL DATA SET DELLE IMMAGINI ORIGINALI, GREZZE\n",
    "\"\"\"\n",
    "RAW_CLASSES_PATH = \"./full_data_set\"\n",
    "\n",
    "\"\"\"\n",
    "PERCORSO DEL DATA SET DOVE VERRANNO SALVATE LE IMMAGINI USATE PER ALLENARE IL MODELLO\n",
    "(80% DELLE IMMAGINI GREZZE)\n",
    "VENGONO SELEZIONATE ALL'INTERNO DELL'ARCHIVIO DELLE IMMAGINI GREZZE\n",
    "QUELLE DEFINITE NEL FILE \"training.csv\"\n",
    "NELLE CELLE SEGUENTI VERRANNO APPLICATE DELLE TECNICHE DI IMAGE PROCESSING SU QUESTE IMMAGINI\n",
    "PER RICAVARE IL VOLTO E RENDERLE UTILIZZABILI DAL MODELLO\n",
    "\"\"\"\n",
    "TRAINING_CLASSES_PATH = \"./training_data_set\"\n",
    "\n",
    "\"\"\"\n",
    "PERCORSO DEL DATA SET DOVE VERRANNO SALVATE LE IMMAGINI USATE DAL MODELLO DURANTE IL PROCESSO DI TRAINING\n",
    "PER VERIFICARE L'EFFICIENZA DELL'ALGORITMO CALCOLATO MEDIANTE BACK PROPAGATION IN QUELL'EPOCA\n",
    "(20% DELLE IMMAGINI GREZZE)\n",
    "VENGONO SELEZIONATE ALL'INTERNO DELL'ARCHIVIO DELLE IMMAGINI GREZZE\n",
    "QUELLE DEFINITE NEL FILE \"validation.csv\"\n",
    "NELLE CELLE SEGUENTI VERRANNO APPLICATE DELLE TECNICHE DI IMAGE PROCESSING SU QUESTE IMMAGINI\n",
    "PER RICAVARE IL VOLTO E RENDERLE UTILIZZABILI DAL MODELLO\n",
    "\"\"\"\n",
    "VALIDATION_CLASSES_PATH = \"./validation_data_set\"\n",
    "\n",
    "\"\"\"\n",
    "PERCORSO DEL DATA SET DOVE SONO PRESENTI LE IMMAGINI CHE L'UTENTE VUOLE SOTTOPORRE ALL'ALGORITMO\n",
    "DOPO ESSERE STATO ALLENATO E QUINDI PRONTO ALL'USO\n",
    "\"\"\"\n",
    "TESTING_CLASSES_PATH = \"./testing_data_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAMITE LA LIBRERIA PYTHON CHE CONTIENE DELLE UTILITY PER SVOLGERE OPERAZIONI SUL SISTEMA OPERATIVO\n",
    "RICAVIAMO LA LISTA DI TUTTE LE CARTELLE CONTENUTE NELL'ARCHIVIO DELLE IMMAGINI GREZZE\n",
    "ESSE RAPPRESENTANO LE CLASSI DI EQUIVALENZA CHE RAPPRESENTANO I VARI INSIEMI\n",
    "CIOE' ESISTE UNA CARTELLA PER OGNI PERSONA\n",
    "OGNI CARTELLA CONTIENE LE IMMAGINI RELATIVE A QUELLA PERSONA\n",
    "\"\"\"\n",
    "peopleFolders = os.listdir(RAW_CLASSES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CASCADING PATTERN\n",
    "\n",
    "DIVERSI PARAMETRI DI CLASSIFICAZIONE SONO INNESTATI\n",
    "LE INFORMAZIONI RACCOLTE DALL'OUTPUT DI UN CLASSIFICATORE\n",
    "VENGONO USATE DA UN ALTRO CLASSIFICATORE PER RIUSCIRE A CLASSIFICARE\n",
    "\n",
    "NEL NOSTRO CASO USIAMO UN MODELLO PRE-ALLENATO\n",
    "PER RICONOSCERE LE FACCE ALL'INTERNO DI UNA IMMAGINE\n",
    "TRAMITE IL PROCESSO DI CONVOLUZIONE\n",
    "E LE INFORMAZIONI SULLE FACCE ESTRATTE VERRANNO USATE PER\n",
    "CLASSIFICARE OGNI FACCIA PER ASSOCIARLA AD UNA PERSONA\n",
    "\"\"\"\n",
    "face_cascade = cv2.CascadeClassifier(\"./haarcascade_frontalface_alt.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FUNZIONE CHE VERRA USATA PER RITAGLIARE TUTTE LE IMMAGINI\n",
    "ALL'INTERNO DI UNA CARTELLA ALL'INTERNO DI UN PERCORSO\n",
    "PARAMETRI:\n",
    "- pathClass: PERCORSO DELLA CARTELLA CHE CONTIENE TUTTE LE IMMAGINI DA RITAGLIARE\n",
    "- sizeImages: DEFINIRE UNA DIMENSIONE UNICA PER TUTTE LE IMMAGINI RITAGLIATE\n",
    "- scaleImage: DI QUANTO L'IMMAGINE DEVE ESSERE RIDOTTA PER OGNI RIDUZIONE DELLA SCALA\n",
    "\n",
    "SCALE PYRAMID PATTERN\n",
    "\n",
    "PER ESSERE IN GRADO DI RICONOSCERE UN PATTERN, UNA PARTICOLARE CARATTERISTICA\n",
    "E' NECESSARIO APPLICARE IL PROCESSO DI CONVOLUZIONE SU DIVERSE SCALE DELLA STESSA IMMAGINE\n",
    "MANTENENDO LA STESSA DIMENSIONE DELLA PATCH CHE ATTRAVERSA L'IMMAGINE PER FARE IL CONTROLLO\n",
    "IN QUESTO MODO IL RICONOSCIMENTO FACCIALE DIVENTA SCALE-INVARIANT\n",
    "CIOE' INSENSIBILE ALLA DIFFERENZA DI SPAZIO OCCUPATO DALLE FACCE ALL'INTERNO DI UNA IMMAGINE\n",
    "\"\"\"\n",
    "def cropAllImages(pathClass, sizeImages, scaleImage=1.3):\n",
    "    # VENGONO SALVATE TUTTI I NOMI DELLE IMMAGINI NELLA CARTELLA O SOTTOCARTELLE\n",
    "    imagesFileNames = [image for image in os.listdir(pathClass) if image.endswith(\".jpg\")]\n",
    "\n",
    "    # RILEVARE PER OGNI IMMAGINE LE FACCE PRESENTI TENENDO CONTO DI DIVERSE SCALE\n",
    "    for imageFileName in imagesFileNames:\n",
    "        # ARRAY DEI PERCORSI DELLE IMMAGINI\n",
    "        imageFileName = os.path.join(pathClass, imageFileName)\n",
    "        image = cv2.imread(imageFileName)\n",
    "        faces = face_cascade.detectMultiScale(image, scaleFactor=scaleImage)\n",
    "\n",
    "        # SE NON CI SONO FACCE, CANCELLA LA FOTO DALL'ARCHIVIO\n",
    "        if len(faces) == 0:\n",
    "            os.remove(imageFileName)\n",
    "        # ALTRIMENTI PER OGNI FACCIA TROVATA DI CUI POSSIEDO LE COORDINATE\n",
    "        # CREA UN'IMMAGINE CON SOLO LA FACCIA RITAGLIATA E SALVALA NELL'ARCHIVIO\n",
    "        else:\n",
    "            for (x, y, width, height) in faces:\n",
    "                # OGNI FACCIA PRESENTE NELLA FOTO VIENE RITAGLIATA\n",
    "                # PER RITAGLIARE UNA FOTO E' NECESSARIO UTILIZZARE UN ALGORITMO DI INTERPOLAZIONE\n",
    "\n",
    "                # ALGORITMO DI INTERPOLAZIONE:\n",
    "                # - UTILE A CALCOLARE I VALORI DEI PIXEL DELLA NUOVA IMMAGINE RITAGLIATA\n",
    "                # - RICAVATI DALL'IMMAGINE ORIGINALE\n",
    "                # - I VALORI DEI PIXEL SONO IL COLORE, L'INTENSITA' PRESENTE IN OGNI PIXEL DELL'IMMAGINE\n",
    "\n",
    "                # INTERPOLAZIONE INTER_AREA\n",
    "                # inv_scale_x, inv_scale_y = ALTEZZA E LARGHEZZA DELL'IMMAGINE DA RITAGLIARE RISPETTO L'ORIGINALE\n",
    "                # scale_x, scale_y = ALTEZZA E LARGHEZZA DELL'IMMAGINE ORIGINALE RISPETTO A QUELLA DA RITAGLIARE\n",
    "                # inv_scale_x = 1./scale_x\n",
    "                # L'IMMAGINE RITAGLIATA VIENE CAMPIONATA UTILIZZANDO UN ALGORITMO\n",
    "                # CHE METTE IN RELAZIONE L'AREA DELL'IMMAGINE ORIGINALE CON LA NUOVA\n",
    "                # QUESTA PUO' ESSERE UTILIZZATA SOLO SE SI VUOLE RIDIMENSIONARE UN'IMMAGINE RITAGLIANDO UNA PARTE DI ESSA\n",
    "                imageResized = cv2.resize(\n",
    "                    image[y: y+height, x: x+width],\n",
    "                    sizeImages,\n",
    "                    interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # POI SALVATA ALL'INTERNO DEL PERCORSO DEL DATA SET (pathClass)\n",
    "                cv2.imwrite(imageFileName, imageResized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PER OGNI CARTELLA CHE RAPPRESENTA UNA PERSONA CHE\n",
    "NE CONTIENE TUTTE LE FOTO CHE LA RAPPRESENTANO\n",
    "APPLICA LA FUNZIONE DESCRITTA NELLA CELLA PRECEDENTE\n",
    "CON UNA DIMENSIONE FISSATA DI 175x175\n",
    "\"\"\"\n",
    "for personFolder in peopleFolders:\n",
    "    cropAllImages(os.path.join(RAW_CLASSES_PATH, personFolder), (175, 175))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CONVERTO I SEGUENTI FILE CSV IN FORMATO JSON:\n",
    "- training.csv\n",
    "- validation.csv\n",
    "CONTENENTI I FILENAME DELLE IMMAGINI CHE DEVONO ESSERE AGGIUNTE:\n",
    "- TRAINING DATA SET\n",
    "- VALIDATION DATA SET\n",
    "\"\"\"\n",
    "jsonTrainingImages = json.loads(open(\"training.csv\", \"r\").read())\n",
    "jsonValidationImages = json.loads(open(\"validation.csv\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CALCOLO IL NUMERO DI IMMAGINI ALL'INTERNO DEI JSON:\n",
    "- jsonTrainingImages\n",
    "- jsonValidationImages\n",
    "SERVIRA' DOPO AVER CREATO IL MODELLO IN FASE DI ALLENAMENTO\n",
    "IN CUI DEVO INDICARE IL NUMERO DI IMMAGINI RISERVATE PER:\n",
    "- TRAINING\n",
    "- VALIDATION\n",
    "CHE OGNI EPOCA DELL'ALLENAMENTO DEVE TENERE IN CONSIDERAZIONE\n",
    "\"\"\"\n",
    "jsonTrainingImagesLength = 0\n",
    "jsonValidationImagesLength = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PER OGNI CARTELLA RISERVATA ALLE IMMAGINI DI UNA SINGOLA PERSONA\n",
    "SALVARE IN UN ARRAY TUTTE LE IMMAGINI RICHIESTE DAL FILE JSON PER:\n",
    "- TRAINING\n",
    "- VALIDATION\n",
    "DA UTILIZZARE PER ALLENARE IL MODELLO A RICONOSCERE QUELLA SINGOLA PERSONA\n",
    "INCREMENTO IL CONTATORE PRECEDENTEMENTE DICHIARATO PER CONTARE IL NUMERO DI IMMAGINI DEDICATE AD ENTRAMBE LE FASI\n",
    "SE NON ESISTE GIA' CREARE UNA CARTELLA DEDICATA AD OGNI PERSONA NEL:\n",
    "- TRAINING DATA SET\n",
    "- VALIDATION DATA SET\n",
    "\"\"\"\n",
    "for personFolder in peopleFolders:\n",
    "    pathClass = os.listdir(os.path.join(RAW_CLASSES_PATH, personFolder))\n",
    "\n",
    "    trainingImagesList = jsonTrainingImages.get(personFolder)\n",
    "    validationImagesList = jsonValidationImages.get(personFolder)\n",
    "\n",
    "    jsonTrainingImagesLength += len(trainingImagesList)\n",
    "    jsonValidationImagesLength += len(validationImagesList)\n",
    "\n",
    "    if not os.path.isdir(os.path.join(TRAINING_CLASSES_PATH, personFolder)):\n",
    "        os.mkdir(os.path.join(TRAINING_CLASSES_PATH, personFolder))\n",
    "    if not os.path.isdir(os.path.join(VALIDATION_CLASSES_PATH, personFolder)):\n",
    "        os.mkdir(os.path.join(VALIDATION_CLASSES_PATH, personFolder))\n",
    "\n",
    "    \"\"\"\n",
    "    DICHIARARE IL PERCORSO PER OGNI IMMAGINE NEL RAW DATA SET\n",
    "    SALVARE L'IMMAGINE TRAMITE LA LIBRERIA \"PIL\"\n",
    "    VERIFICARE L'INTEGRITA' DELL'IMMAGINE\n",
    "    SE L'IMMAGINE E' DI TIPO (.jpg) ED E' IN FORMATO (RGB)\n",
    "    COPIA L'IMMAGINE DEL VOLTO ALL'INTERNO DEL TRAINING DATA SET\n",
    "    \"\"\"\n",
    "    for trainingImage in trainingImagesList:\n",
    "        pathImage = os.path.join(RAW_CLASSES_PATH, personFolder, trainingImage)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(pathImage)\n",
    "            image.verify()\n",
    "\n",
    "            if trainingImage.endswith(\".jpg\") and image.mode == \"RGB\":\n",
    "                copyfile(pathImage, os.path.join(TRAINING_CLASSES_PATH, personFolder, trainingImage))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        #    print(\"Failed to open image: \", personFolder, trainingImage)\n",
    "\n",
    "    \"\"\"\n",
    "    DICHIARARE IL PERCORSO PER OGNI IMMAGINE NEL RAW DATA SET\n",
    "    SALVARE L'IMMAGINE TRAMITE LA LIBRERIA \"PIL\"\n",
    "    VERIFICARE L'INTEGRITA' DELL'IMMAGINE\n",
    "    SE L'IMMAGINE E' DI TIPO (.jpg) ED E' IN FORMATO (RGB)\n",
    "    COPIA L'IMMAGINE DEL VOLTO ALL'INTERNO DEL VALIDATION DATA SET\n",
    "    \"\"\"\n",
    "    for validationImage in validationImagesList:\n",
    "        pathImage = os.path.join(RAW_CLASSES_PATH, personFolder, validationImage)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(pathImage)\n",
    "            image.verify()\n",
    "\n",
    "            if validationImage.endswith(\".jpg\") and image.mode == \"RGB\":\n",
    "                copyfile(pathImage, os.path.join(VALIDATION_CLASSES_PATH, personFolder, validationImage))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        #   print(\"Failed to open image: \", personFolder, validationImage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 5 classes.\n",
      "Found 75 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ImageDataGenerator\n",
    "\n",
    "CLASSE CHE PERMETTE DI FARE OPERAZIONI DI DATA AUGMENTATION\n",
    "CIOE' FACENDO DELLE ELABORAZIONI SULLE IMMAGINI IN BATCH\n",
    "PERMETTE DI PREVENIRE L'OVERFITTING\n",
    "EVITANDO DI FOCALIZZARSI SU DETTAGLI NON PARTICOLARMENTE RILEVANTI PER IL RICONOSCIMENTO\n",
    "PERTANTO PERMETTE UNA MIGLIORE GENERALIZZAZIONE DELLA PERSONA\n",
    "PERCHE' SI DANNO PIU' VERSIONI DI UN'IMMAGINE DELLA STESSA PERSONA\n",
    "VERRANNO FATTE DURANTE IL TRAINING\n",
    "\n",
    "TRASFORMAZIONI:\n",
    "- RESCALE DEI VALORI DEI PIXEL, NORMALIZZAZIONE\n",
    "SERVE A GENERALIZZARE IL VALORE DEL COLORE, DELL'INTENSITA' DI OGNI PIXEL\n",
    "I VALORI RGB: NUMERI INTERI TRA [0, 255]\n",
    "I VALORI NORMALIZZATI: NUMERI RAZIONALI TRA [0, 1]\n",
    "\n",
    "NORMALIZZANDO I VALORI DEI PIXEL LA DISCESA DEL GRADIENTE\n",
    "ALL'INTERNO DELLA FUNZIONE A PIU' VARIABILI DEFINITA DALLA RETE NEURALE\n",
    "IN CUI WEIGHTS, BIAS SONO LE INCOGNITE\n",
    "E' PIU' VELOCE E CONVERGE PIU' FACILMENTE\n",
    "\"\"\"\n",
    "editedImageGenerator = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "\"\"\"\n",
    "CREAZIONE DI UN IMAGEGENERATOR CHE NORMALIZZA LE FOTO DEI VOLTI DEDICATE AL TRAINING\n",
    "- path: PERCORSO DOVE SI TROVANO LE IMMAGINI DA NORMALIZZARE\n",
    "- target_size: RISOLUZIONE DELLE IMMAGINI CHE DEVONO ESSERE RESTITUITE IN OUTPUT\n",
    "- batch_size: NUMERO DI FOTO PER IL TRAINING CHE VENGONO NORMALIZZATE CONTEMPORANEAMENTE\n",
    "- class_mode:\n",
    "  SE CI SONO SOLO 2 CLASSI E' BINARY\n",
    "  SE CI SONO TANTE CLASSI E' CATEGORICAL (ABBIAMO UN NUMERO INDETERMINATO DI PERSONE)\n",
    "\"\"\"\n",
    "trainingImagesGenerator = editedImageGenerator.flow_from_directory(TRAINING_CLASSES_PATH,\n",
    "                                                                   target_size=(175, 175),\n",
    "                                                                   batch_size=128,\n",
    "                                                                   class_mode=\"categorical\")\n",
    "\n",
    "editedImageGenerator = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "\"\"\"\n",
    "CREAZIONE DI UN IMAGEGENERATOR CHE NORMALIZZA LE FOTO DEI VOLTI DEDICATE AL VALIDATION\n",
    "- path: PERCORSO DOVE SI TROVANO LE IMMAGINI DA NORMALIZZARE\n",
    "- target_size: RISOLUZIONE DELLE IMMAGINI CHE DEVONO ESSERE RESTITUITE IN OUTPUT\n",
    "- batch_size: NUMERO DI FOTO PER IL VALIDATION CHE VENGONO NORMALIZZATE CONTEMPORANEAMENTE\n",
    "- class_mode:\n",
    "  SE CI SONO SOLO 2 CLASSI E' BINARY\n",
    "  SE CI SONO TANTE CLASSI E' CATEGORICAL (ABBIAMO UN NUMERO INDETERMINATO DI PERSONE)\n",
    "\"\"\"\n",
    "validationImagesGenerator = editedImageGenerator.flow_from_directory(VALIDATION_CLASSES_PATH,\n",
    "                                                                     target_size=(175, 175),\n",
    "                                                                     batch_size=32,\n",
    "                                                                     class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INSIEME DELLE CLASSI DEFINITE NEL GENERATOR, INVERTENDO L'ORDINE\n",
    "- indexPerson: INDICE DELLA CLASSE NELL'ARRAY\n",
    "- namePerson: CLASSE, NOME DI UNA PERSONA\n",
    "\"\"\"\n",
    "classes = {indexPerson: namePerson for namePerson, indexPerson in trainingImagesGenerator.class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KERAS SI APPOGGIA SU UN BACKEND CHE E' TENSORFLOW\n",
    "IL BACKEND E' COMPOSTO DA SESSIONI\n",
    "CHE SONO L'AMBIENTE IN CUI:\n",
    "- OPERAZIONI DEL MODELLO VENGONO ESEGUITE\n",
    "- TENSORI SONO VALUTATI MEDIANTI FUNZIONI PER LA METRICA E IL CALCOLO DELLE PERDITE\n",
    "\n",
    "  Resets all state generated by Keras.\n",
    "\n",
    "  Keras manages a global state, which it uses to implement the Functional\n",
    "  model-building API and to uniquify autogenerated layer names.\n",
    "\n",
    "  If you are creating many models in a loop, this global state will consume\n",
    "  an increasing amount of memory over time, and you may want to clear it.\n",
    "  Calling `clear_session()` releases the global state: this helps avoid clutter\n",
    "  from old models and layers, especially when memory is limited.\n",
    "\"\"\"\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\"\"\"\n",
    "CREAZIONE DI UN MODELLO SEQUENZIALE IN KERAS PER IL FACE RECOGNITION, SONO PRESENTI:\n",
    "\n",
    "CONVOLUTIONAL LAYER 2D, PARAMETRI:\n",
    "    - NUMERO DI OPERAZIONI (NEURONI)\n",
    "    - DIMENSIONE DEL FILTRO\n",
    "    - FUNZIONE PER L'ATTIVAZIONE DEI NEURONI\n",
    "    - PADDING ALL'INTERNO DELL'IMMAGINE SI (SAME) O NO (VALID)\n",
    "      SE SI LA CONVOLUZIONE VIENE FATTA ANCHE SUI BORDI\n",
    "      L'IMMAGINE DI OUTPUT PRESERVERA' LA STESSA DIMENSIONE DELL'INPUT\n",
    "    - DIMENSIONE DI TUTTI I TENSORI 3D (IMMAGINI) (WIDTH, HEIGHT, RGB)\n",
    "\n",
    "MAX POOLING LAYER 2D, PARAMETRI:\n",
    "    - DIMENSIONE FILTRO TENSORE SU CUI PRENDERE IL MASSIMO\n",
    "\n",
    "FLATTEN LAYER\n",
    "- IL LAYER PRECEDENTE (2D, 3D) VIENE FATTO DIVENTARE UNIDIMENSIONALE (1D)\n",
    "\n",
    "DROPOUT LAYER, PARAMETRI:\n",
    "- RATE: FRAZIONE CHE INDICA LA QUANTITA' DI INPUT DA AZZERARE\n",
    "\n",
    "DENSE LAYER, PARAMETRI:\n",
    "- NUMERO DI OPERAZIONI (NEURONI)\n",
    "- FUNZIONE DI ATTIVAZIONE (ReLU, Softmax)\n",
    "\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Conv2D(32,\n",
    "                           (3, 3),\n",
    "                           activation=\"relu\",\n",
    "                           padding=\"same\",\n",
    "                           input_shape=(175, 175, 3)),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(32,\n",
    "                           (3, 3),\n",
    "                           activation=\"relu\",\n",
    "                           padding=\"same\"),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64,\n",
    "                           (3, 3),\n",
    "                           activation=\"relu\",\n",
    "                           padding=\"same\"),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64,\n",
    "                           (3, 3),\n",
    "                           activation=\"relu\",\n",
    "                           padding=\"same\"),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64,\n",
    "                           (3, 3),\n",
    "                           activation=\"relu\",\n",
    "                           padding=\"same\"),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(32,\n",
    "                           (3, 3),\n",
    "                           activation=\"relu\",\n",
    "                           padding=\"same\"),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Dense(len(trainingImagesGenerator.class_indices),\n",
    "                          activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 175, 175, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 87, 87, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 87, 87, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 43, 43, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 21, 21, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 32)          18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 253,317\n",
      "Trainable params: 253,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# STAMPA A VIDEO UNA STRINGA CHE DESCRIVE IL MODELLO PRECEDENTEMENTE CREATO\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SI OCCUPA DELLA CONFIGURAZIONE SU COME FARE IL TRAINING PER IL MODELLO DEFINITO, PARAMETRI:\n",
    "\n",
    "- ADAM OPTIMIZER\n",
    "    METODO DI DISCESA DEL GRADIENTE STOCASTICO BASATO SULLA STIMA ADATTIVA DEI MOMENTI DEL PRIMO E DEL SECONDO ORDINE\n",
    "    ESSO INDICA L'ALGORITMO DA UTILIZZARE PER IL BACK PROPAGATION\n",
    "    CIOE' COME L'ALGORITMO DEVE INTERPRETARE I VALORI DI OUTPUT\n",
    "    E IN CHE DIREZIONE ESSI DEBBANO ESSERE MODIFICATI NELLA PROSSIMA EPOCA\n",
    "    DEFINISCE LA CURVA DI APPRENDIMENTO\n",
    "\n",
    "- CATEGORICAL CROSSENTROPY LOSS\n",
    "    ALGORITMO DI ATTIVAZIONE \"SOFTMAX\" + FUNZIONE PER IL CALCOLO DELLA PERDITA \"CROSS ENTROPY\"\n",
    "    ADDESTRIAMO UNA CNN A FORNIRCI UNA PROBABILITÀ SU N CLASSI\n",
    "    CHE UNA CERTA IMMAGINE FORNITA IN INPUT ABBIA UNA PARTICOLARE CARATTERISTICA\n",
    "    VIENE UTILIZZATO PER LA CLASSIFICAZIONE QUANDO CI SONO PIÙ DI 2 CLASSI\n",
    "\n",
    "- ACCURACY METRICS\n",
    "    PARAMETRO CON CUI VIENE VALUTATO L'OUTPUT DI UN MODELLO IN UN'EPOCA\n",
    "    DURANTE IL TRAINING ED IL TESTING\n",
    "    CHE SI CERCA DI FAR AUMENTARE IL PIU' POSSIBILE PER OGNI EPOCA\n",
    "    IN QUESTO CASO L'ACCURATEZZA DELL'OUTPUT DEL MODELLO RISPETTO ALLA RISPOSTA IDEALE\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "ALGORITMO OPTIMIZER\n",
    "\n",
    "OGNI VOLTA CHE UNA RETE NEURALE:\n",
    "- FINISCE DI PASSARE DEI VALORI ATTRAVERSO LA RETE\n",
    "- DI GENERARE RISULTATI DI PREVISIONE\n",
    "- DI CALCOLARE UN VALORE CHE INDICA LA DISTANZA TRA I VALORI DI ATTIVAZIONE DI OUTPUT DELL'OUTPUT LAYER E I VALORI IDEALI\n",
    "    (MEDIANTE LA FUNZIONE DI LOSS)\n",
    "\n",
    "DEVE DECIDERE COME UTILIZZARE LA DIFFERENZA TRA I RISULTATI OTTENUTI E I VALORI CHE SA ESSERE VERI\n",
    "PER REGOLARE I PESI SUI NODI IN MODO CHE LA RETE SI AVVICINI SEMPRE DI PIÙ VERSO UNA SOLUZIONE\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "FUNZIONI DI PERDITA (LOSS FUNCTION)\n",
    "\n",
    "FUNZIONE CHE CALCOLA LA QUANTITÀ CHE UN MODELLO\n",
    "DOVREBBE CERCARE DI RIDURRE AL MINIMO DURANTE L'ADDESTRAMENTO\n",
    "\n",
    "QUESTA QUANTITA' RAPPRESENTA UN VALORE CHE INDICA LA DISTANZA CHE C'E' TRA:\n",
    "- TUTTI I VALORI DI ATTIVAZIONE DEL LAYER DI OUTPUT\n",
    "- TUTTI I VALORI IDEALI CHE IL MODELLO DOVREBBE RESTITUIRE PER QUEGLI OUTPUT\n",
    "\"\"\"\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "earlyStopping = tf.keras.callbacks.EarlyStopping(patience=12)\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(\"faces.h5\",\n",
    "                                                     save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 3/11 [=======>......................] - ETA: 14s - loss: 1.6130 - accuracy: 0.1665WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2200 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 11 batches). You may need to use the repeat() function when building your dataset.\n",
      "11/11 [==============================] - 8s 471ms/step - loss: 1.6023 - accuracy: 0.2250 - val_loss: 1.5278 - val_accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALLENA IL MODELLO PER UN NUMERO DEFINITO DI EPOCHE\n",
    "\n",
    "EPOCA -> ITERAZIONI FATTE SU UN TRAINING DATA SET DEL MODELLO CHE VIENE REGOLATO DI VOLTA IN VOLTA\n",
    "\n",
    "PARAMETRI:\n",
    "- training_data\n",
    "- validation_data\n",
    "- epochs\n",
    "    NUMERO DEFINITO DI EPOCHE\n",
    "- callbacks\n",
    "    CALLBACKS CHE DEVONO ESSERE CHIAMATE DURANTE IL TRAINING\n",
    "    FUNZIONI CHE VENGONO ESEGUITE DURANTE IL TRAINING\n",
    "- steps_per_epoch\n",
    "- validation_steps\n",
    "\"\"\"\n",
    "historyModel = model.fit(trainingImagesGenerator,\n",
    "                    validation_data = validationImagesGenerator,\n",
    "                    epochs=200,\n",
    "                    callbacks=[earlyStopping, modelCheckpoint],\n",
    "                    steps_per_epoch = math.ceil(jsonTrainingImagesLength//128),\n",
    "                    validation_steps = math.ceil(jsonValidationImagesLength//32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'historyModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-43-67a13cd84829>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhistoryModel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"accuracy\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhistoryModel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"val_accuracy\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'historyModel' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(historyModel.history[\"accuracy\"])\n",
    "plt.plot(historyModel.history[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(historyModel.history[\"loss\"])\n",
    "plt.plot(historyModel.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"faces.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cropAllImages(TESTING_CLASSES_PATH, (175, 175))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imagesFileNamesList = [image for image in os.listdir(\"./\") if image.endswith(\".jpg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for imageFileName in imagesFileNamesList:\n",
    "    image = tf.keras.preprocessing.image.load_img(imageFileName,\n",
    "                                                  target_size=(175, 175))\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image) / 255.\n",
    "\n",
    "    plt.imshow(image)\n",
    "\n",
    "    index = np.argmax(model.predict(image[tf.newaxis, ...]))\n",
    "\n",
    "    print(classes[index])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Adam Sandler',\n",
       " 1: 'Adele',\n",
       " 2: 'Andy Samberg',\n",
       " 3: 'Arnold Schwarzenegger',\n",
       " 4: 'Ben Stiller'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAMPA LE CLASSI DEL MODELLO\n",
    "classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}